{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try Basic Tokenizer training on a small text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe.basic2 import BasicTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now load all uzbek nasr books in cyrillic and train Basic Tokenizer on Big Uzbek Nasr Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = pathlib.Path(r\"C:\\Users\\amrul\\programming\\nlp_related\\datasets\\whole_ocr_collection-20231208T081731Z-001\\whole_ocr_collection\")\n",
    "files = [file for file in data_folder.iterdir() if \"txt\" in file.suffix]\n",
    "print(f\"There are {len(files)} txt files in {data_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cyrillic files only\n",
    "non_cyrillic_filenames=['abdulla_chimirzayev_hayot_yog_dulari_hikoyalar_whole_ocr.txt',\n",
    " 'abdulla_qahhor_hikoyalar_1933_whole_ocr.txt',\n",
    " 'abdulla_qahhor_qanotsiz_chittak_1937_whole_ocr.txt',\n",
    " 'abdulla_qahhor_qotilning_tug_ilishi_1933_whole_ocr.txt',\n",
    " 'abdulla_qodiriy_jinlar_bazmi_hikoyalar_whole_ocr.txt',\n",
    " 'baxtiyor_omon_boburning_bolaligi_hikoyalar_start_100_whole_ocr.txt',\n",
    " 'bibi_robia_saidova_parvoz_hikoyalar_start_100_whole_ocr.txt',\n",
    " 'cho_lpon_kecha_va_kunduz_roman_start_100_whole_ocr.txt',\n",
    " 'dinora_rahimova_qishloqdagi_buvijonim_qissa_start_100_whole_ocr.txt',\n",
    " 'erkin_a_zam_ertak_bilan_xayrlashuv_qissalar_va_hikoyalar_start_100_whole_ocr.txt',\n",
    " 'hasan_muxtorov_armon_qissa_start_200_whole_ocr.txt',\n",
    " 'https_n_ziyouz_com_https_www_phoca_cz_phocadownload_whole_ocr.txt',\n",
    " 'husaynxon_orifiy_ayanchli_qismat_qissa_va_hikoyalar_start_200_whole_ocr.txt',\n",
    " 'ibrohim_rahim_fidoyilar_roman_start_200_whole_ocr.txt',\n",
    " 'ilhom_zoyir_yuz_tillo_mojarosi_roman_start_200_whole_ocr.txt',\n",
    " 'inomjon_abdiyev_arslon_yelkasidagi_xazina_qissa_start_200_whole_ocr.txt',\n",
    " 'kimsan_mashrab_turon_o_g_li_devona_mashrab_badia_start_200_whole_ocr.txt',\n",
    " 'latif_mahmudiv_sevgi_desam_hikoyalar_start_200_whole_ocr.txt',\n",
    " 'mamatqul_hazratqulov_eshiklar_ochiq_qissa_start_200_whole_ocr.txt',\n",
    " 'mirza_karim_mohlaroyim_qissa_start_300_whole_ocr.txt',\n",
    " 'muhammad_ismoil_bahorning_eng_so_nggi_lolasi_start_300_whole_ocr.txt',\n",
    " 'muhammad_ismoil_zabarjad_qissa_start_300_whole_ocr.txt',\n",
    " 'murod_muhammad_do_st_galatepaga_qaytish_qissa_start_300_whole_ocr.txt',\n",
    " 'muyassar_tilovova_burgutlar_hikoyalar_start_300_whole_ocr.txt',\n",
    " 'normurod_norqobilov_g_animlar_qissa_start_300_whole_ocr.txt',\n",
    " 'normurod_norqobilov_temur_g_ori_1999_start_300_whole_ocr.txt',\n",
    " 'nurulla_chori_bo_ron_tingan_kecha_hikoyalar_start_400_whole_ocr.txt',\n",
    " 'oqiljon_husan_tog_da_o_sgan_bola_roman_start_400_whole_ocr.txt',\n",
    " 'oybek_navoiy_roman_start_400_whole_ocr.txt',\n",
    " 'pirimqul_qodirov_shohruh_va_gavharshod_roman_start_400_whole_ocr.txt',\n",
    " 'shuhrat_yetim_boshin_silaganlar_hikoya_va_qissalar_start_500_whole_ocr.txt',\n",
    " 'sotim_avaz_temurg_ozi_to_ra_start_600_whole_ocr.txt',\n",
    " 'tog_ay_murod_ot_kishnagan_oqshom_qissalar_2006_start_600_whole_ocr.txt',\n",
    " 'tog_ay_murod_yulduzlar_mangu_yonadi_qissalar_start_600_whole_ocr.txt',\n",
    " 'xayriddin_sultonov_ko_ngil_ozodadur_qissa_start_600_whole_ocr.txt',\n",
    " 'xayriddin_sultonov_saodat_sohili_qissa_start_600_whole_ocr.txt',\n",
    " 'xayriddin_sultonov_saodat_sohili_start_600_whole_ocr.txt',\n",
    " 'xudoyberdi_to_xtaboyev_qasoskorning_oltin_boshi_roman_start_600_whole_ocr.txt',\n",
    " 'xurshid_davron_bibixonim_qissasi_hikoya_va_qissalar_start_600_whole_ocr.txt',\n",
    " 'xurshid_davron_tarixiy_hikoyalar_start_600_whole_ocr.txt',\n",
    " 'zohir_a_lam_afandining_qirq_bir_pashshasi_qissa_start_700_whole_ocr.txt']\n",
    "\n",
    "cyr_files = [file for file in files if file.name not in non_cyrillic_filenames]\n",
    "print(f\"extracted {len(cyr_files)} cyrillic files out of {len(files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "big_text = \"\"\n",
    "\n",
    "with tqdm(total=len(cyr_files)) as pbar:\n",
    "    for file in cyr_files:\n",
    "        big_text = f\"{big_text}\\n{file.read_text(encoding='utf-8')}\"\n",
    "        pbar.set_description(f\"Read {file.name[:40]:40}\")\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = list(set(big_text))\n",
    "print(f\"there are {len(all_chars)} distinct characters in big text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampytools.list_utils import get_list_diff\n",
    "\n",
    "cyrillic_text=\"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюяЎҒҚҲўғқҳ\"\n",
    "cyrillic_chars = list(cyrillic_text)\n",
    "exclude_chars = get_list_diff(all_chars,cyrillic_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer(all_chars, exclude_chars)\n",
    "print(f\"Tokenizer vocab length before loading model file : {len(tokenizer.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moment of truth train tokenizer on big text. For the start let's go with hundred thousand mergees\n",
    "num_merges = 10000\n",
    "tokenizer.train(\n",
    "    big_text, 256 + num_merges, verbose=True, prompt_interval=5000\n",
    ")  # 256 are the byte tokens, then do num_merges merges\n",
    "\n",
    "print(tokenizer.encode(big_text[:1000]))\n",
    "\n",
    "print(tokenizer.decode([258, 259, 260, 261, 262]))\n",
    "\n",
    "tokenizer.save(\"uzbek_nasr\")\n",
    "# writes two files: uzbek_nasr.model (for loading) and uzbek_nasr.vocab (for viewing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_312_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
